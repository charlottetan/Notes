{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"bigtable/bigtable/","text":"Bigtable A distributed storage system for managing structured data that is designed to scale to a very large size. Data Model Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded Implementation Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry) Refinements locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"bigtable"},{"location":"bigtable/bigtable/#bigtable","text":"A distributed storage system for managing structured data that is designed to scale to a very large size.","title":"Bigtable"},{"location":"bigtable/bigtable/#data-model","text":"Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded","title":"Data Model"},{"location":"bigtable/bigtable/#implementation","text":"Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry)","title":"Implementation"},{"location":"bigtable/bigtable/#refinements","text":"locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"Refinements"},{"location":"word2vec/word2vec/","text":"Efficient Estimation of Word Representations in Vector Space Novel model architectures for computing continuous vector representations of words from very large data sets. Background Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\"). Previous Work O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific. NNLM(feedforward neural network language model) As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary. New Log-linear Models CBOW Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V) Skip-gram Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"word2vec"},{"location":"word2vec/word2vec/#efficient-estimation-of-word-representations-in-vector-space","text":"Novel model architectures for computing continuous vector representations of words from very large data sets.","title":"Efficient Estimation of Word Representations in Vector Space"},{"location":"word2vec/word2vec/#background","text":"Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\").","title":"Background"},{"location":"word2vec/word2vec/#previous-work","text":"O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.","title":"Previous Work"},{"location":"word2vec/word2vec/#nnlmfeedforward-neural-network-language-model","text":"As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary.","title":"NNLM(feedforward neural network language model)"},{"location":"word2vec/word2vec/#new-log-linear-models","text":"","title":"New Log-linear Models"},{"location":"word2vec/word2vec/#cbow","text":"Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V)","title":"CBOW"},{"location":"word2vec/word2vec/#skip-gram","text":"Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"Skip-gram"},{"location":"zfs/zfs/","text":"Zettabyte File System A new file system with strong data integrity, simple administration and immense capacity. Features redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data Concepts On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself. Implementation - SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects SPA (Storage Pool Allocator) verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks) DMU (Data Management Unit) when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes ZPL (ZFS Posix Layer) use intent log to avoid losing writes before system crashes","title":"zfs"},{"location":"zfs/zfs/#zettabyte-file-system","text":"A new file system with strong data integrity, simple administration and immense capacity.","title":"Zettabyte File System"},{"location":"zfs/zfs/#features","text":"redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data","title":"Features"},{"location":"zfs/zfs/#concepts","text":"On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself.","title":"Concepts"},{"location":"zfs/zfs/#implementation","text":"- SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects","title":"Implementation"},{"location":"zfs/zfs/#spa-storage-pool-allocator","text":"verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks)","title":"SPA (Storage Pool Allocator)"},{"location":"zfs/zfs/#dmu-data-management-unit","text":"when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes","title":"DMU (Data Management Unit)"},{"location":"zfs/zfs/#zpl-zfs-posix-layer","text":"use intent log to avoid losing writes before system crashes","title":"ZPL (ZFS Posix Layer)"}]}