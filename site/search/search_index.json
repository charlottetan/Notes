{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\"And the universe said you are the universe tasting itself, talking to itself, reading its own code.\"","title":"Home"},{"location":"aurora/aurora/","text":"Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases A cloud-native relational database service for OLTP workloads. Introduction We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Key idea: the log is the database; any page that the storage system materializes are simply a cache of log application. Architecture To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. : Extra Notes In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.","title":"aurora"},{"location":"aurora/aurora/#amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases","text":"A cloud-native relational database service for OLTP workloads.","title":"Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases"},{"location":"aurora/aurora/#introduction","text":"We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Key idea: the log is the database; any page that the storage system materializes are simply a cache of log application.","title":"Introduction"},{"location":"aurora/aurora/#architecture","text":"To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. :","title":"Architecture"},{"location":"aurora/aurora/#extra-notes","text":"In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.","title":"Extra Notes"},{"location":"bigtable/bigtable/","text":"Bigtable A distributed storage system for managing structured data that is designed to scale to a very large size. Data Model Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded Implementation Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry) Refinements locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"bigtable"},{"location":"bigtable/bigtable/#bigtable","text":"A distributed storage system for managing structured data that is designed to scale to a very large size.","title":"Bigtable"},{"location":"bigtable/bigtable/#data-model","text":"Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded","title":"Data Model"},{"location":"bigtable/bigtable/#implementation","text":"Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry)","title":"Implementation"},{"location":"bigtable/bigtable/#refinements","text":"locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"Refinements"},{"location":"borg/borg/","text":"Large-scale cluster management at Google with Borg A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization. Basics Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint. Architecture A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets. Features Scalability To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score. Availability Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down. Isolation Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"borg"},{"location":"borg/borg/#large-scale-cluster-management-at-google-with-borg","text":"A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization.","title":"Large-scale cluster management at Google with Borg"},{"location":"borg/borg/#basics","text":"Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint.","title":"Basics"},{"location":"borg/borg/#architecture","text":"A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets.","title":"Architecture"},{"location":"borg/borg/#features","text":"","title":"Features"},{"location":"borg/borg/#scalability","text":"To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score.","title":"Scalability"},{"location":"borg/borg/#availability","text":"Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down.","title":"Availability"},{"location":"borg/borg/#isolation","text":"Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"Isolation"},{"location":"d-stream/d-stream/","text":"Discretized Streams: Fault-Tolerant Streaming Computation at Scale A new stream processing model with efficient fault recovery and straggler mitigation. Background There is a need for streaming computation models that can scale transparently to large clusters, where faults and stragglers are inevitable. Most existing systems are based on continuous operator model(like Storm). The stateful nature of operators and nondeterminism of records make it hard to provide efficient fault recovery. Two main approaches, replication(2x hardware) or upstream backup(long replay time), are both expensive. Neither approach handles stragglers -- in replication, synchronization protocols slow both down replicas; in upstream backup, a straggler must be treated as a failure. We seek a system design with scalability to hundreds of nodes, minimal cost, second-scale latency and second-scale fault/straggler recovery. Discretized Streams D-Stream model treats a streaming computation as a series of deterministic batch computations on small time intervals. Users define programs by manipulating D-Stream objects; a D-Stream is a sequence of RDDs. The consistency semantics are clear because time is naturally discretized into intervals and each interval's output RDDs reflect all inputs received up to that interval. D-Streams place records into input datasets based on the time when each records arrives at the system. Grouping records by external timestamps is not natively supported. System Architecture Spark Streaming consists of three components, (1) a master to track D-Stream lineage and schedules tasks, (2) worker nodes to receive/store/process data and computed RDDs and (3) a client library to send data into system. To decide when to process a new interval, nodes synchronize clocks via NTP and send the master a list of received block IDs in each interval when it ends. The master then starts launching tasks for that interval without further synchronization. Spark Streaming divides computations into short, stateless, deterministic tasks, each of which may run on any node in the cluster. Moving computations to another machine is hard in traditional streaming systems with rigid topologies. To support streaming, Spark Streaming implements changes/optimizations like timestep pipelining, lineage cutoff, master recovery, etc. Fault and Straggler Recovery Parallel recovery: when a node fails, the state RDD partitions and running tasks on that node can be recomputed in parallel on other nodes. Straggler mitigation: run speculative backup copies of slow tasks like in batch systems. Master recovery: workers can simply connect to a new master since there is no problem if a given RDD is computed twice(it is fine to lose some running tasks).","title":"d-stream"},{"location":"d-stream/d-stream/#discretized-streams-fault-tolerant-streaming-computation-at-scale","text":"A new stream processing model with efficient fault recovery and straggler mitigation.","title":"Discretized Streams: Fault-Tolerant Streaming Computation at Scale"},{"location":"d-stream/d-stream/#background","text":"There is a need for streaming computation models that can scale transparently to large clusters, where faults and stragglers are inevitable. Most existing systems are based on continuous operator model(like Storm). The stateful nature of operators and nondeterminism of records make it hard to provide efficient fault recovery. Two main approaches, replication(2x hardware) or upstream backup(long replay time), are both expensive. Neither approach handles stragglers -- in replication, synchronization protocols slow both down replicas; in upstream backup, a straggler must be treated as a failure. We seek a system design with scalability to hundreds of nodes, minimal cost, second-scale latency and second-scale fault/straggler recovery.","title":"Background"},{"location":"d-stream/d-stream/#discretized-streams","text":"D-Stream model treats a streaming computation as a series of deterministic batch computations on small time intervals. Users define programs by manipulating D-Stream objects; a D-Stream is a sequence of RDDs. The consistency semantics are clear because time is naturally discretized into intervals and each interval's output RDDs reflect all inputs received up to that interval. D-Streams place records into input datasets based on the time when each records arrives at the system. Grouping records by external timestamps is not natively supported.","title":"Discretized Streams"},{"location":"d-stream/d-stream/#system-architecture","text":"Spark Streaming consists of three components, (1) a master to track D-Stream lineage and schedules tasks, (2) worker nodes to receive/store/process data and computed RDDs and (3) a client library to send data into system. To decide when to process a new interval, nodes synchronize clocks via NTP and send the master a list of received block IDs in each interval when it ends. The master then starts launching tasks for that interval without further synchronization. Spark Streaming divides computations into short, stateless, deterministic tasks, each of which may run on any node in the cluster. Moving computations to another machine is hard in traditional streaming systems with rigid topologies. To support streaming, Spark Streaming implements changes/optimizations like timestep pipelining, lineage cutoff, master recovery, etc.","title":"System Architecture"},{"location":"d-stream/d-stream/#fault-and-straggler-recovery","text":"Parallel recovery: when a node fails, the state RDD partitions and running tasks on that node can be recomputed in parallel on other nodes. Straggler mitigation: run speculative backup copies of slow tasks like in batch systems. Master recovery: workers can simply connect to a new master since there is no problem if a given RDD is computed twice(it is fine to lose some running tasks).","title":"Fault and Straggler Recovery"},{"location":"databus/databus/","text":"Databus: Linkedin\u2019s Scalable Consistent Change Data Capture Platform A source-agnostic distributed change data capture system of LinkedIn's data processing pipeline. Background Data systems are categorized as primary and derived data stores. A requirement from derived data stores is to capture, flow and process primary data changes. There are two families of solutions, (1) application-driven dual writes that lets application writes multiple data systems in parallel and (2) database log mining that extracts changes from primary data store's transaction log. We choose (2) because it avoids complex coordination protocol for consistency and has single source-of-truth. Architecture Overview Logical Components: fetcher to extract changes, log store to cache change stream, snapshot store to store a moving snapshot of change stream and subscription client to pull changes and surface them to applications. relay process = fetcher + in-memory log store bootstrap service = persistent log store + snapshot store Databus is externally-clocked. Each change set is annotated with a monotonically-increasing system change number (SCN) assigned by data source. Databus favors pull over push model because we need to keep SCN timelines for a large number consumers and it simplifies error handling. Relay The changes extracted by fetcher are serialized into binary format independent of sources (Avro). They are grouped within transaction boundaries, annotated with SCN and buffered in the transient log. The relay does not maintain consumer-related states. Subscription client maintains progress checkpoint and and pass on each pull request. To deploy a cluster of processes, we can let either (1) all relays independently connecting the data source or (2) one relay as leader and the rest followers pull changes from the leader. The change data is stored in a long-lived in-memory circular buffer using the on-wire serialization format. This enables high-throughput bulk-writes when serving pull requests and minimizes GC pressure (performance variance). The trade-off here is on-wire format needs more CPU resources for filtering. A skip list index is implemented on top of the buffer to efficiently serve pull requests to scan changes from a particular SCN. Consistency windows are marked explicitly so consumers can differentiate the cases of no updates and of no updates matching the filter. The end-of-window markers can also contain additional metadata like checksums. Bootstrap Service For consumers fallen behind a lot, it is more efficient to catch up with a snapshot of compacted changes, i.e. only the latest state of affected rows. Bootstrap service database has a persistent log store and a snapshot store because: New changes are applied to the snapshot store while the consumer applications are reading (it is not practical to lock the snapshot store), so bootstrap service will replay changes after the snapshot read started for consistency. It ensures bootstrap service has enough write throughput to keep up the data source since appending to the log store is much cheaper than building the snapshot store.","title":"databus"},{"location":"databus/databus/#databus-linkedins-scalable-consistent-change-data-capture-platform","text":"A source-agnostic distributed change data capture system of LinkedIn's data processing pipeline.","title":"Databus: Linkedin\u2019s Scalable Consistent Change Data Capture Platform"},{"location":"databus/databus/#background","text":"Data systems are categorized as primary and derived data stores. A requirement from derived data stores is to capture, flow and process primary data changes. There are two families of solutions, (1) application-driven dual writes that lets application writes multiple data systems in parallel and (2) database log mining that extracts changes from primary data store's transaction log. We choose (2) because it avoids complex coordination protocol for consistency and has single source-of-truth.","title":"Background"},{"location":"databus/databus/#architecture","text":"","title":"Architecture"},{"location":"databus/databus/#overview","text":"Logical Components: fetcher to extract changes, log store to cache change stream, snapshot store to store a moving snapshot of change stream and subscription client to pull changes and surface them to applications. relay process = fetcher + in-memory log store bootstrap service = persistent log store + snapshot store Databus is externally-clocked. Each change set is annotated with a monotonically-increasing system change number (SCN) assigned by data source. Databus favors pull over push model because we need to keep SCN timelines for a large number consumers and it simplifies error handling.","title":"Overview"},{"location":"databus/databus/#relay","text":"The changes extracted by fetcher are serialized into binary format independent of sources (Avro). They are grouped within transaction boundaries, annotated with SCN and buffered in the transient log. The relay does not maintain consumer-related states. Subscription client maintains progress checkpoint and and pass on each pull request. To deploy a cluster of processes, we can let either (1) all relays independently connecting the data source or (2) one relay as leader and the rest followers pull changes from the leader. The change data is stored in a long-lived in-memory circular buffer using the on-wire serialization format. This enables high-throughput bulk-writes when serving pull requests and minimizes GC pressure (performance variance). The trade-off here is on-wire format needs more CPU resources for filtering. A skip list index is implemented on top of the buffer to efficiently serve pull requests to scan changes from a particular SCN. Consistency windows are marked explicitly so consumers can differentiate the cases of no updates and of no updates matching the filter. The end-of-window markers can also contain additional metadata like checksums.","title":"Relay"},{"location":"databus/databus/#bootstrap-service","text":"For consumers fallen behind a lot, it is more efficient to catch up with a snapshot of compacted changes, i.e. only the latest state of affected rows. Bootstrap service database has a persistent log store and a snapshot store because: New changes are applied to the snapshot store while the consumer applications are reading (it is not practical to lock the snapshot store), so bootstrap service will replay changes after the snapshot read started for consistency. It ensures bootstrap service has enough write throughput to keep up the data source since appending to the log store is much cheaper than building the snapshot store.","title":"Bootstrap Service"},{"location":"dynamo/dynamo/","text":"Dynamo: Amazon\u2019s Highly Available Key-value Store A highly available key-value storage system for \"always-on\" experience. Background The Amazon platform is built on top of tens of thousands of server and network components, where there are always a small but significant number of components failing at any given time. Some applications like shopping cart need always-available storage technologies for customer experience. Introduction Dynamo provides only simple key-value interface; no operations span multiple data items. Unlike traditional commercial systems putting importance on consistency, Dynamo sacrifices consistency under certain failure scenarios for availability. Dynamo uses a synthesis of well known techniques to achieve scalability and availability: System Architecture Consistent hashing: To scale incrementally, Dynamo uses consistent hashing to partition data. The principle advantage is that arrival/departure of a node only affects immediate neighbors. Virtual node : Dynamo introduces the concept of virtual nodes to balance the load when membership changes and account for heterogeneity in the physical infrastructure (virtual nodes on same physical node are skipped in replication). Data Versioning : Dynamo uses vector clocks(list of <node, counter> pairs) to capture the causality between different versions of the same object. If Dynamo can't resolve divergent versions, it will return all objects and let applications resolve the conflicts. Sloppy quorum and hinted handoff: Dynamo uses quorum-based consistency protocol(R+W>N), but does not ensure strict quorum membership. When a node A is temporarily unavailable, another node B will help maintain the replica and deliver the replica to A when detecting A has recovered. Replica synchronization: Dynamo anti-entropy protocol uses hash trees to reduce the amount of data need to be transferred to detect replica inconsistencies. Gossip protocol: Each node contacts a random peer every second and two nodes reconcile their persisted membership change histories and views of failure state. To prevent logical partitions, some seed nodes are known to all nodes.","title":"dynamo"},{"location":"dynamo/dynamo/#dynamo-amazons-highly-available-key-value-store","text":"A highly available key-value storage system for \"always-on\" experience.","title":"Dynamo: Amazon\u2019s Highly Available Key-value Store"},{"location":"dynamo/dynamo/#background","text":"The Amazon platform is built on top of tens of thousands of server and network components, where there are always a small but significant number of components failing at any given time. Some applications like shopping cart need always-available storage technologies for customer experience.","title":"Background"},{"location":"dynamo/dynamo/#introduction","text":"Dynamo provides only simple key-value interface; no operations span multiple data items. Unlike traditional commercial systems putting importance on consistency, Dynamo sacrifices consistency under certain failure scenarios for availability. Dynamo uses a synthesis of well known techniques to achieve scalability and availability:","title":"Introduction"},{"location":"dynamo/dynamo/#system-architecture","text":"Consistent hashing: To scale incrementally, Dynamo uses consistent hashing to partition data. The principle advantage is that arrival/departure of a node only affects immediate neighbors. Virtual node : Dynamo introduces the concept of virtual nodes to balance the load when membership changes and account for heterogeneity in the physical infrastructure (virtual nodes on same physical node are skipped in replication). Data Versioning : Dynamo uses vector clocks(list of <node, counter> pairs) to capture the causality between different versions of the same object. If Dynamo can't resolve divergent versions, it will return all objects and let applications resolve the conflicts. Sloppy quorum and hinted handoff: Dynamo uses quorum-based consistency protocol(R+W>N), but does not ensure strict quorum membership. When a node A is temporarily unavailable, another node B will help maintain the replica and deliver the replica to A when detecting A has recovered. Replica synchronization: Dynamo anti-entropy protocol uses hash trees to reduce the amount of data need to be transferred to detect replica inconsistencies. Gossip protocol: Each node contacts a random peer every second and two nodes reconcile their persisted membership change histories and views of failure state. To prevent logical partitions, some seed nodes are known to all nodes.","title":"System Architecture"},{"location":"espresso/espresso/","text":"On Brewing Fresh Espresso: LinkedIn\u2019s Distributed Data Serving Platform A timeline-consistent document-oriented distributed database to address Linkedin's requirements for primary store. Background Timeline Consistency : events are applied in the same order on all replicas. Linkedin used to have a single RDBMS with user data tables; two derived data systems, for full-text search and relationship graph traversal, are kept up-to-date by Databus. RDBMS has pain points like scaling and schema management, while most Linkedin primary data does not require full RDBMS functionality. Voldemort, a Dynamo-like data store, is increasingly being used for primary data, but key-value model does not support secondary indexing very efficiently. Data Model Linkedin mainly deals with two forms of relationships, nested entities and independent entities. Applications need atomicity constraints for updating nested entities, but not for independent entities. Espresso uses a hierarchical data model to model nested entities efficiently. Independent entities are modeled as disjoint entities with change capture stream to materialize relationships. The data hierarchy is composed of document(smallest data unit with primary key), table(collection of like-schema-ed documents), document group(collection of documents with common partition key) and database. All documents within a database are partitioned with the same partitioning strategy. Document group is a logical concept and can span across tables; it is the largest unit with transactionality support(same partition). System Architecture Espresso is composed of: clients and routers, storage nodes, relays(Databus) and cluster manager(Helix). Storage nodes maintain both base data and local secondary indexes. To achieve read-after-write consistency, updates are applied transactionality to base data and local secondary indexes. Data is over-partitioned to reduce load during cluster expansion. For each partition, Helix assigns one storage node as master and rest as slaves. Committed changes are pulled into Databus via local transaction log and slave replicas consume change streams from Databus. (Local) Secondary Index Local secondary indexes(for document groups) must be updated in real time, while global secondary indexes(for independent entities) can be updated asynchronously. The first approach is based on Lucene. We create per-collection small indexes to reduce memory footprint and store them in MySQL to achieve transactionality. The second approach is called Prefix Index. We prefix each term in the index with the collection key(equivalent to having per-collection small indexes); terms are store on a B+ tree structure and corresponding document lists are stored in MySQL. This avoids the overhead of frequently opening/closing indexes and re-indexing the whole document to update indexes.","title":"espresso"},{"location":"espresso/espresso/#on-brewing-fresh-espresso-linkedins-distributed-data-serving-platform","text":"A timeline-consistent document-oriented distributed database to address Linkedin's requirements for primary store.","title":"On Brewing Fresh Espresso: LinkedIn\u2019s Distributed Data Serving Platform"},{"location":"espresso/espresso/#background","text":"Timeline Consistency : events are applied in the same order on all replicas. Linkedin used to have a single RDBMS with user data tables; two derived data systems, for full-text search and relationship graph traversal, are kept up-to-date by Databus. RDBMS has pain points like scaling and schema management, while most Linkedin primary data does not require full RDBMS functionality. Voldemort, a Dynamo-like data store, is increasingly being used for primary data, but key-value model does not support secondary indexing very efficiently.","title":"Background"},{"location":"espresso/espresso/#data-model","text":"Linkedin mainly deals with two forms of relationships, nested entities and independent entities. Applications need atomicity constraints for updating nested entities, but not for independent entities. Espresso uses a hierarchical data model to model nested entities efficiently. Independent entities are modeled as disjoint entities with change capture stream to materialize relationships. The data hierarchy is composed of document(smallest data unit with primary key), table(collection of like-schema-ed documents), document group(collection of documents with common partition key) and database. All documents within a database are partitioned with the same partitioning strategy. Document group is a logical concept and can span across tables; it is the largest unit with transactionality support(same partition).","title":"Data Model"},{"location":"espresso/espresso/#system-architecture","text":"Espresso is composed of: clients and routers, storage nodes, relays(Databus) and cluster manager(Helix). Storage nodes maintain both base data and local secondary indexes. To achieve read-after-write consistency, updates are applied transactionality to base data and local secondary indexes. Data is over-partitioned to reduce load during cluster expansion. For each partition, Helix assigns one storage node as master and rest as slaves. Committed changes are pulled into Databus via local transaction log and slave replicas consume change streams from Databus.","title":"System Architecture"},{"location":"espresso/espresso/#local-secondary-index","text":"Local secondary indexes(for document groups) must be updated in real time, while global secondary indexes(for independent entities) can be updated asynchronously. The first approach is based on Lucene. We create per-collection small indexes to reduce memory footprint and store them in MySQL to achieve transactionality. The second approach is called Prefix Index. We prefix each term in the index with the collection key(equivalent to having per-collection small indexes); terms are store on a B+ tree structure and corresponding document lists are stored in MySQL. This avoids the overhead of frequently opening/closing indexes and re-indexing the whole document to update indexes.","title":"(Local) Secondary Index"},{"location":"f1/f1/","text":"F1: A Distributed SQL Database That Scales A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL. Basics MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency. Architecture F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers. Features Hierarchical Schema Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser). Non-blocking Schema Changes A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies. Optimistic Transaction F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions. Change History Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache. Query Processing F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"f1"},{"location":"f1/f1/#f1-a-distributed-sql-database-that-scales","text":"A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL.","title":"F1: A Distributed SQL Database That Scales"},{"location":"f1/f1/#basics","text":"MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency.","title":"Basics"},{"location":"f1/f1/#architecture","text":"F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers.","title":"Architecture"},{"location":"f1/f1/#features","text":"","title":"Features"},{"location":"f1/f1/#hierarchical-schema","text":"Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser).","title":"Hierarchical Schema"},{"location":"f1/f1/#non-blocking-schema-changes","text":"A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies.","title":"Non-blocking Schema Changes"},{"location":"f1/f1/#optimistic-transaction","text":"F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions.","title":"Optimistic Transaction"},{"location":"f1/f1/#change-history","text":"Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache.","title":"Change History"},{"location":"f1/f1/#query-processing","text":"F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"Query Processing"},{"location":"flume/flume/","text":"FlumeJava: Easy, Efficient Data-Parallel Pipelines The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. VS MapReduce Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed. Basics Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> . Optimizer ParallelDo Fusion Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) . MSCR Fusion MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces. Strategy The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs. Executor Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"flume"},{"location":"flume/flume/#flumejava-easy-efficient-data-parallel-pipelines","text":"The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines.","title":"FlumeJava: Easy, Efficient Data-Parallel Pipelines"},{"location":"flume/flume/#vs-mapreduce","text":"Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed.","title":"VS MapReduce"},{"location":"flume/flume/#basics","text":"Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> .","title":"Basics"},{"location":"flume/flume/#optimizer","text":"","title":"Optimizer"},{"location":"flume/flume/#paralleldo-fusion","text":"Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) .","title":"ParallelDo Fusion"},{"location":"flume/flume/#mscr-fusion","text":"MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces.","title":"MSCR Fusion"},{"location":"flume/flume/#strategy","text":"The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs.","title":"Strategy"},{"location":"flume/flume/#executor","text":"Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"Executor"},{"location":"gfs/gfs/","text":"The Google File System A scalable distributed file system for large distributed data-intensive applications. Introduction The design is driven by key observations different from earlier file system assumptions, including frequent component failures, huge files and that most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer). Architecture A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. System Interactions For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries,.Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Master Operations GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability and network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations.","title":"gfs"},{"location":"gfs/gfs/#the-google-file-system","text":"A scalable distributed file system for large distributed data-intensive applications.","title":"The Google File System"},{"location":"gfs/gfs/#introduction","text":"The design is driven by key observations different from earlier file system assumptions, including frequent component failures, huge files and that most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer).","title":"Introduction"},{"location":"gfs/gfs/#architecture","text":"A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages.","title":"Architecture"},{"location":"gfs/gfs/#system-interactions","text":"For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries,.Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical).","title":"System Interactions"},{"location":"gfs/gfs/#master-operations","text":"GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability and network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations.","title":"Master Operations"},{"location":"google-news/google-news/","text":"Google News Personalization: Scalable Online Collaborative Filtering Collaborative filtering for generating personalized recommendations for users of Google News. Background Algorithms MinHash PLSI Covisitation System Structure","title":"Google News Personalization: Scalable Online Collaborative Filtering"},{"location":"google-news/google-news/#google-news-personalization-scalable-online-collaborative-filtering","text":"Collaborative filtering for generating personalized recommendations for users of Google News.","title":"Google News Personalization: Scalable Online Collaborative Filtering"},{"location":"google-news/google-news/#background","text":"","title":"Background"},{"location":"google-news/google-news/#algorithms","text":"","title":"Algorithms"},{"location":"google-news/google-news/#minhash","text":"","title":"MinHash"},{"location":"google-news/google-news/#plsi","text":"","title":"PLSI"},{"location":"google-news/google-news/#covisitation","text":"","title":"Covisitation"},{"location":"google-news/google-news/#system-structure","text":"","title":"System Structure"},{"location":"mapreduce/mapreduce/","text":"MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets. Introduction Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Execution A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks. Fault Tolerance For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry. Miscellaneous Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers.","title":"mapreduce"},{"location":"mapreduce/mapreduce/#mapreduce-simplified-data-processing-on-large-clusters","text":"A programming model and an associated implementation for processing and generating large data sets.","title":"MapReduce: Simplified Data Processing on Large Clusters"},{"location":"mapreduce/mapreduce/#introduction","text":"Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key.","title":"Introduction"},{"location":"mapreduce/mapreduce/#execution","text":"A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks.","title":"Execution"},{"location":"mapreduce/mapreduce/#fault-tolerance","text":"For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry.","title":"Fault Tolerance"},{"location":"mapreduce/mapreduce/#miscellaneous","text":"Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers.","title":"Miscellaneous"},{"location":"millwheel/millwheel/","text":"MillWheel: Fault-Tolerant Stream Processing at Internet Scale A framework for building streaming systems with fault tolerance, persistent state and scalability. Background Other streaming systems do not provide the combination of fault tolerance, versatility and scalability. A monotonically increasing low watermark, which indicates all data up to a given timestamp has been received, is useful to distinguish whether there is no data or data is delayed. System Overview MillWheel is a graph of user-define transformations on input data that produces output data. Inputs/outputs are represented by (key, value, timestamp) triples. User code can access a per-key, per-computation persistent state for per-key aggregation. Delivery Guarantee: all internal updates within the MillWheel framework resulting from record processing are atomically checkpointed per-key and records are delivered exactly-once. Concepts A computation subscribes to zero or more input streams and publishes one or more output streams. Persistent states are per-key opaque byte strings stored in Bigtable/Spanner. Consumer specifies per-stream key extraction functions. Computation code(containing application logic) is invoked upon receipt of input data. Processing is serialized per-key but can be parallelized over distinct keys. Low watermark of a computation A is defined as min(oldest work of A, low watermark of C: C outputs to A)\u200b. Low watermark values are seeded by injectors that send data into MillWheel from external systems. Fault Tolerance Delivery Guarantee System assigns unique IDs to all records at production time. Upon receipt of an input record, MillWheel checks the record against deduplication data(bloom filter + backing store). For strong productions, MillWheel checkpoints produced records before delivery in the same atomic write as state modification. When a process restarts, the checkpoints are scanned into memory and replayed. For weak productions, MillWheel broadcasts downstream deliveries optimistically prior to persisting state. MillWheel selectively checkpoints a small percentage of straggler productions to prevent them from occupying undue resources in the sender. State Manipulation To avoid inconsistencies in persistent state, all per-key updates are wrapped in a single atomic operation. To address zombie writers and stale writes in network, MillWheel guarantees that for a given key, only a single worker can write to that key at a particular point in time. It is implemented by attaching a sequencer token to each write and let backing store check for validity. Single-writer guarantee is critical to state consistency and low watermark correctness. System Implementation A replicated master divides each computation into a set of owned lexicographical key intervals and assigns intervals to a set of machines. It can move/split/merge intervals. Each interval is assigned a unique sequencer and is invalidated whenever the interval is changed(for single-writer guarantee). Low watermarks are tracked by a central authority. Workers compute low watermark updates and report to the central authority, so the central authority's low watermark values are always as conservative as workers'.","title":"millwheel"},{"location":"millwheel/millwheel/#millwheel-fault-tolerant-stream-processing-at-internet-scale","text":"A framework for building streaming systems with fault tolerance, persistent state and scalability.","title":"MillWheel: Fault-Tolerant Stream Processing at Internet Scale"},{"location":"millwheel/millwheel/#background","text":"Other streaming systems do not provide the combination of fault tolerance, versatility and scalability. A monotonically increasing low watermark, which indicates all data up to a given timestamp has been received, is useful to distinguish whether there is no data or data is delayed.","title":"Background"},{"location":"millwheel/millwheel/#system-overview","text":"MillWheel is a graph of user-define transformations on input data that produces output data. Inputs/outputs are represented by (key, value, timestamp) triples. User code can access a per-key, per-computation persistent state for per-key aggregation. Delivery Guarantee: all internal updates within the MillWheel framework resulting from record processing are atomically checkpointed per-key and records are delivered exactly-once.","title":"System Overview"},{"location":"millwheel/millwheel/#concepts","text":"A computation subscribes to zero or more input streams and publishes one or more output streams. Persistent states are per-key opaque byte strings stored in Bigtable/Spanner. Consumer specifies per-stream key extraction functions. Computation code(containing application logic) is invoked upon receipt of input data. Processing is serialized per-key but can be parallelized over distinct keys. Low watermark of a computation A is defined as min(oldest work of A, low watermark of C: C outputs to A)\u200b. Low watermark values are seeded by injectors that send data into MillWheel from external systems.","title":"Concepts"},{"location":"millwheel/millwheel/#fault-tolerance","text":"","title":"Fault Tolerance"},{"location":"millwheel/millwheel/#delivery-guarantee","text":"System assigns unique IDs to all records at production time. Upon receipt of an input record, MillWheel checks the record against deduplication data(bloom filter + backing store). For strong productions, MillWheel checkpoints produced records before delivery in the same atomic write as state modification. When a process restarts, the checkpoints are scanned into memory and replayed. For weak productions, MillWheel broadcasts downstream deliveries optimistically prior to persisting state. MillWheel selectively checkpoints a small percentage of straggler productions to prevent them from occupying undue resources in the sender.","title":"Delivery Guarantee"},{"location":"millwheel/millwheel/#state-manipulation","text":"To avoid inconsistencies in persistent state, all per-key updates are wrapped in a single atomic operation. To address zombie writers and stale writes in network, MillWheel guarantees that for a given key, only a single worker can write to that key at a particular point in time. It is implemented by attaching a sequencer token to each write and let backing store check for validity. Single-writer guarantee is critical to state consistency and low watermark correctness.","title":"State Manipulation"},{"location":"millwheel/millwheel/#system-implementation","text":"A replicated master divides each computation into a set of owned lexicographical key intervals and assigns intervals to a set of machines. It can move/split/merge intervals. Each interval is assigned a unique sequencer and is invalidated whenever the interval is changed(for single-writer guarantee). Low watermarks are tracked by a central authority. Workers compute low watermark updates and report to the central authority, so the central authority's low watermark values are always as conservative as workers'.","title":"System Implementation"},{"location":"predicting-clicks/predicting-clicks/","text":"Predicting Clicks: Estimating the Click-Through Rate for New Ads Use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. Background The key task for a search engine advertising system is to determine what advertisements should be displayed, and in what order, for each query that the search engine receives. Assume the probability of an ad is clicked is independent of its position and the probability of an ad is viewed only depends on position. P(click|ad,pos)=p(click|ad, seen) \\cdot p(seen|pos) . Then CTR is defined as p(click|ad, seen) . For ads that has been displayed enough times(up to 200-300 search pages), CTR can be estimated based on historical information, but this doesn't work for new ads. Model Basics The goal is to create a model to predict initial CTR for new ads. Training and testing data is split on advertiser level, so we can consider each ad and account as completely novel to the system. Use Logistics regression and cross-entropy loss function. For each feature f_i , add derived features log(f_i+1) and fi^2 . Feature values more than five standard deviations from the mean are truncated. Features Term CTR: the CTR of other ads with same or related bid terms. Ad quality feature set manual features: appearance, attention capture, reputation, landing page quality, relevance. unigram features: one-hot encoding of most common 10K words in ad title and body. Order specificity feature set: capture how targeted an order with the category entropy of bid terms. Search data feature set: the approximate frequency of bid term occurring on the Web and users query for the bid term. Others Although there is overlapping between features, we want to Include as many feature sets as possible for robustness in adversarial situations. Improvements: making the CTR estimation dependent on user query, adding human judges as new source of information, making the model time-dependent.","title":"predicting-clicks"},{"location":"predicting-clicks/predicting-clicks/#predicting-clicks-estimating-the-click-through-rate-for-new-ads","text":"Use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads.","title":"Predicting Clicks: Estimating the Click-Through Rate for New Ads"},{"location":"predicting-clicks/predicting-clicks/#background","text":"The key task for a search engine advertising system is to determine what advertisements should be displayed, and in what order, for each query that the search engine receives. Assume the probability of an ad is clicked is independent of its position and the probability of an ad is viewed only depends on position. P(click|ad,pos)=p(click|ad, seen) \\cdot p(seen|pos) . Then CTR is defined as p(click|ad, seen) . For ads that has been displayed enough times(up to 200-300 search pages), CTR can be estimated based on historical information, but this doesn't work for new ads.","title":"Background"},{"location":"predicting-clicks/predicting-clicks/#model-basics","text":"The goal is to create a model to predict initial CTR for new ads. Training and testing data is split on advertiser level, so we can consider each ad and account as completely novel to the system. Use Logistics regression and cross-entropy loss function. For each feature f_i , add derived features log(f_i+1) and fi^2 . Feature values more than five standard deviations from the mean are truncated.","title":"Model Basics"},{"location":"predicting-clicks/predicting-clicks/#features","text":"Term CTR: the CTR of other ads with same or related bid terms. Ad quality feature set manual features: appearance, attention capture, reputation, landing page quality, relevance. unigram features: one-hot encoding of most common 10K words in ad title and body. Order specificity feature set: capture how targeted an order with the category entropy of bid terms. Search data feature set: the approximate frequency of bid term occurring on the Web and users query for the bid term.","title":"Features"},{"location":"predicting-clicks/predicting-clicks/#others","text":"Although there is overlapping between features, we want to Include as many feature sets as possible for robustness in adversarial situations. Improvements: making the CTR estimation dependent on user query, adding human judges as new source of information, making the model time-dependent.","title":"Others"},{"location":"rdd/rdd/","text":"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing A distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Background Current distributed computing frameworks(like MapReduce) lack abstractions for leveraging distributed memory, so they are inefficient for applications that need to reuse intermediate results(iterative machine learning, interactive data mining, etc). Current abstractions for cluster in-memory storage(like Piccolo) provide interface with fine-grained updates. They have to replicate data or log updates across machines for fault tolerance, which is expensive for data-intensive workloads. RDD Model An RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on (1) data in stable storage or (2) other RDDs. RDD provides interface based on coarse-grained transformations that apply the same operation to many data items, which allows efficient fault tolerance by logging the transformations used to build a dataset(lineage) rather than actual data. RDD is not suitable for applications that needs to make asynchronous fine-grained updates to shared states(incremental web crawler, etc). Implementation An RDD is represented as (1) a set of partitions, (2) a set of dependencies of parent RDDs, (3) a function for computing the dataset from parents and (4) metadata about partitioning scheme and data placement. There are two types of dependencies, narrow(like map or filter) and wide(like join). Narrow dependencies can be more efficiently executed and recovered after failure. The scheduler examines the RDD's lineage graph to build a DAG of stages to execute. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of stages are shuffle operations or any already computed partitions. The scheduler assigns tasks to machines based on data locality using delay scheduling. A task is sent to node that has the partition available in memory or is preferred location for HDFS. If a task fails, the scheduler rerun it on another node and resubmit tasks for parent partitions if necessary. Use LRU eviction policy at the level of RDDs to manage the limited memory available. Checkpointing is useful for RDDs with long lineage graphs containing wide dependencies. The read-only nature of RDDs make them simpler to checkpoint than distributed shared memory.","title":"rdd"},{"location":"rdd/rdd/#resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing","text":"A distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner.","title":"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing"},{"location":"rdd/rdd/#background","text":"Current distributed computing frameworks(like MapReduce) lack abstractions for leveraging distributed memory, so they are inefficient for applications that need to reuse intermediate results(iterative machine learning, interactive data mining, etc). Current abstractions for cluster in-memory storage(like Piccolo) provide interface with fine-grained updates. They have to replicate data or log updates across machines for fault tolerance, which is expensive for data-intensive workloads.","title":"Background"},{"location":"rdd/rdd/#rdd-model","text":"An RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on (1) data in stable storage or (2) other RDDs. RDD provides interface based on coarse-grained transformations that apply the same operation to many data items, which allows efficient fault tolerance by logging the transformations used to build a dataset(lineage) rather than actual data. RDD is not suitable for applications that needs to make asynchronous fine-grained updates to shared states(incremental web crawler, etc).","title":"RDD Model"},{"location":"rdd/rdd/#implementation","text":"An RDD is represented as (1) a set of partitions, (2) a set of dependencies of parent RDDs, (3) a function for computing the dataset from parents and (4) metadata about partitioning scheme and data placement. There are two types of dependencies, narrow(like map or filter) and wide(like join). Narrow dependencies can be more efficiently executed and recovered after failure. The scheduler examines the RDD's lineage graph to build a DAG of stages to execute. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of stages are shuffle operations or any already computed partitions. The scheduler assigns tasks to machines based on data locality using delay scheduling. A task is sent to node that has the partition available in memory or is preferred location for HDFS. If a task fails, the scheduler rerun it on another node and resubmit tasks for parent partitions if necessary. Use LRU eviction policy at the level of RDDs to manage the limited memory available. Checkpointing is useful for RDDs with long lineage graphs containing wide dependencies. The read-only nature of RDDs make them simpler to checkpoint than distributed shared memory.","title":"Implementation"},{"location":"spanner/spanner/","text":"Spanner: Google\u2019s Globally-Distributed Database Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. Introduction Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world. System Architecture A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups). TruetTme Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters. Concurrency Control Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions.","title":"spanner"},{"location":"spanner/spanner/#spanner-googles-globally-distributed-database","text":"Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database.","title":"Spanner: Google\u2019s Globally-Distributed Database"},{"location":"spanner/spanner/#introduction","text":"Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world.","title":"Introduction"},{"location":"spanner/spanner/#system-architecture","text":"A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups).","title":"System Architecture"},{"location":"spanner/spanner/#truettme","text":"Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters.","title":"TruetTme"},{"location":"spanner/spanner/#concurrency-control","text":"Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions.","title":"Concurrency Control"},{"location":"storm/storm/","text":"Storm @Twitter A real-time fault-tolerant and distributed stream data processing system. Basics Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka. Architecture Overview Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism. Nimbus Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks. Workers Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue. Processing Semantics Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"storm"},{"location":"storm/storm/#storm-twitter","text":"A real-time fault-tolerant and distributed stream data processing system.","title":"Storm @Twitter"},{"location":"storm/storm/#basics","text":"Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka.","title":"Basics"},{"location":"storm/storm/#architecture","text":"","title":"Architecture"},{"location":"storm/storm/#overview","text":"Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism.","title":"Overview"},{"location":"storm/storm/#nimbus","text":"Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks.","title":"Nimbus"},{"location":"storm/storm/#workers","text":"Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue.","title":"Workers"},{"location":"storm/storm/#processing-semantics","text":"Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"Processing Semantics"},{"location":"tao/tao/","text":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph A geographically distributed data store that provides efficient and timely access to the social graph. Background The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency. Data Model and API Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list. Architecture Basics TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job. Scaling Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency. Consistency After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency. Fault Tolerance Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"tao"},{"location":"tao/tao/#tao-facebooks-distributed-data-store-for-the-social-graph","text":"A geographically distributed data store that provides efficient and timely access to the social graph.","title":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph"},{"location":"tao/tao/#background","text":"The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency.","title":"Background"},{"location":"tao/tao/#data-model-and-api","text":"Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list.","title":"Data Model and API"},{"location":"tao/tao/#architecture","text":"","title":"Architecture"},{"location":"tao/tao/#basics","text":"TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job.","title":"Basics"},{"location":"tao/tao/#scaling","text":"Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency.","title":"Scaling"},{"location":"tao/tao/#consistency","text":"After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency.","title":"Consistency"},{"location":"tao/tao/#fault-tolerance","text":"Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"Fault Tolerance"},{"location":"word2vec/word2vec/","text":"Efficient Estimation of Word Representations in Vector Space Novel model architectures for computing continuous vector representations of words from very large data sets. Background Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\"). Previous Work O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific. NNLM(feedforward neural network language model) As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary. New Log-linear Models CBOW Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V) Skip-gram Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"word2vec"},{"location":"word2vec/word2vec/#efficient-estimation-of-word-representations-in-vector-space","text":"Novel model architectures for computing continuous vector representations of words from very large data sets.","title":"Efficient Estimation of Word Representations in Vector Space"},{"location":"word2vec/word2vec/#background","text":"Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\").","title":"Background"},{"location":"word2vec/word2vec/#previous-work","text":"O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.","title":"Previous Work"},{"location":"word2vec/word2vec/#nnlmfeedforward-neural-network-language-model","text":"As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary.","title":"NNLM(feedforward neural network language model)"},{"location":"word2vec/word2vec/#new-log-linear-models","text":"","title":"New Log-linear Models"},{"location":"word2vec/word2vec/#cbow","text":"Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V)","title":"CBOW"},{"location":"word2vec/word2vec/#skip-gram","text":"Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"Skip-gram"},{"location":"zfs/zfs/","text":"Zettabyte File System A new file system with strong data integrity, simple administration and immense capacity. Features redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data Concepts On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself. Implementation - SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects SPA (Storage Pool Allocator) verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks) DMU (Data Management Unit) when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes ZPL (ZFS Posix Layer) use intent log to avoid losing writes before system crashes","title":"zfs"},{"location":"zfs/zfs/#zettabyte-file-system","text":"A new file system with strong data integrity, simple administration and immense capacity.","title":"Zettabyte File System"},{"location":"zfs/zfs/#features","text":"redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data","title":"Features"},{"location":"zfs/zfs/#concepts","text":"On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself.","title":"Concepts"},{"location":"zfs/zfs/#implementation","text":"- SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects","title":"Implementation"},{"location":"zfs/zfs/#spa-storage-pool-allocator","text":"verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks)","title":"SPA (Storage Pool Allocator)"},{"location":"zfs/zfs/#dmu-data-management-unit","text":"when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes","title":"DMU (Data Management Unit)"},{"location":"zfs/zfs/#zpl-zfs-posix-layer","text":"use intent log to avoid losing writes before system crashes","title":"ZPL (ZFS Posix Layer)"},{"location":"zookeeper/zookeeper/","text":"ZooKeeper: Wait-free coordination for Internet-scale systems A service for coordinating processes of distributed applications. Basics ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"zookeeper"},{"location":"zookeeper/zookeeper/#zookeeper-wait-free-coordination-for-internet-scale-systems","text":"A service for coordinating processes of distributed applications.","title":"ZooKeeper: Wait-free coordination for Internet-scale systems"},{"location":"zookeeper/zookeeper/#basics","text":"ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1.","title":"Basics"},{"location":"zookeeper/zookeeper/#service-overview","text":"","title":"Service Overview"},{"location":"zookeeper/zookeeper/#znode","text":"ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB).","title":"znode"},{"location":"zookeeper/zookeeper/#client-api","text":"ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages.","title":"Client API"},{"location":"zookeeper/zookeeper/#implementation","text":"ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"Implementation"}]}