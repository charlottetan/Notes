{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A future is not given to you. It is something you must take for yourself.","title":"Home"},{"location":"bigtable/bigtable/","text":"Bigtable A distributed storage system for managing structured data that is designed to scale to a very large size. Data Model Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded Implementation Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry) Refinements locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"bigtable"},{"location":"bigtable/bigtable/#bigtable","text":"A distributed storage system for managing structured data that is designed to scale to a very large size.","title":"Bigtable"},{"location":"bigtable/bigtable/#data-model","text":"Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded","title":"Data Model"},{"location":"bigtable/bigtable/#implementation","text":"Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry)","title":"Implementation"},{"location":"bigtable/bigtable/#refinements","text":"locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"Refinements"},{"location":"borg/borg/","text":"Large-scale cluster management at Google with Borg A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization. Basics Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint. Architecture A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets. Features Scalability To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score. Availability Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down. Isolation Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"borg"},{"location":"borg/borg/#large-scale-cluster-management-at-google-with-borg","text":"A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization.","title":"Large-scale cluster management at Google with Borg"},{"location":"borg/borg/#basics","text":"Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint.","title":"Basics"},{"location":"borg/borg/#architecture","text":"A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets.","title":"Architecture"},{"location":"borg/borg/#features","text":"","title":"Features"},{"location":"borg/borg/#scalability","text":"To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score.","title":"Scalability"},{"location":"borg/borg/#availability","text":"Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down.","title":"Availability"},{"location":"borg/borg/#isolation","text":"Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"Isolation"},{"location":"f1/f1/","text":"F1: A Distributed SQL Database That Scales A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL. Basics MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency. Architecture F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers. Features Hierarchical Schema Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser). Non-blocking Schema Changes A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies. Optimistic Transaction F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions. Change History Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache. Query Processing F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"f1"},{"location":"f1/f1/#f1-a-distributed-sql-database-that-scales","text":"A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL.","title":"F1: A Distributed SQL Database That Scales"},{"location":"f1/f1/#basics","text":"MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency.","title":"Basics"},{"location":"f1/f1/#architecture","text":"F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers.","title":"Architecture"},{"location":"f1/f1/#features","text":"","title":"Features"},{"location":"f1/f1/#hierarchical-schema","text":"Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser).","title":"Hierarchical Schema"},{"location":"f1/f1/#non-blocking-schema-changes","text":"A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies.","title":"Non-blocking Schema Changes"},{"location":"f1/f1/#optimistic-transaction","text":"F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions.","title":"Optimistic Transaction"},{"location":"f1/f1/#change-history","text":"Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache.","title":"Change History"},{"location":"f1/f1/#query-processing","text":"F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"Query Processing"},{"location":"flume/flume/","text":"FlumeJava: Easy, Efficient Data-Parallel Pipelines The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. VS MapReduce Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed. Basics Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> . Optimizer ParallelDo Fusion Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) . MSCR Fusion MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces. Strategy The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs. Executor Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"flume"},{"location":"flume/flume/#flumejava-easy-efficient-data-parallel-pipelines","text":"The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines.","title":"FlumeJava: Easy, Efficient Data-Parallel Pipelines"},{"location":"flume/flume/#vs-mapreduce","text":"Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed.","title":"VS MapReduce"},{"location":"flume/flume/#basics","text":"Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> .","title":"Basics"},{"location":"flume/flume/#optimizer","text":"","title":"Optimizer"},{"location":"flume/flume/#paralleldo-fusion","text":"Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) .","title":"ParallelDo Fusion"},{"location":"flume/flume/#mscr-fusion","text":"MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces.","title":"MSCR Fusion"},{"location":"flume/flume/#strategy","text":"The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs.","title":"Strategy"},{"location":"flume/flume/#executor","text":"Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"Executor"},{"location":"storm/storm/","text":"Storm @Twitter A real-time fault-tolerant and distributed stream data processing system. Basics Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka. Architecture Overview Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism. Nimbus Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks. Workers Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue. Processing Semantics Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"storm"},{"location":"storm/storm/#storm-twitter","text":"A real-time fault-tolerant and distributed stream data processing system.","title":"Storm @Twitter"},{"location":"storm/storm/#basics","text":"Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka.","title":"Basics"},{"location":"storm/storm/#architecture","text":"","title":"Architecture"},{"location":"storm/storm/#overview","text":"Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism.","title":"Overview"},{"location":"storm/storm/#nimbus","text":"Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks.","title":"Nimbus"},{"location":"storm/storm/#workers","text":"Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue.","title":"Workers"},{"location":"storm/storm/#processing-semantics","text":"Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"Processing Semantics"},{"location":"word2vec/word2vec/","text":"Efficient Estimation of Word Representations in Vector Space Novel model architectures for computing continuous vector representations of words from very large data sets. Background Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\"). Previous Work O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific. NNLM(feedforward neural network language model) As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary. New Log-linear Models CBOW Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V) Skip-gram Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"word2vec"},{"location":"word2vec/word2vec/#efficient-estimation-of-word-representations-in-vector-space","text":"Novel model architectures for computing continuous vector representations of words from very large data sets.","title":"Efficient Estimation of Word Representations in Vector Space"},{"location":"word2vec/word2vec/#background","text":"Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\").","title":"Background"},{"location":"word2vec/word2vec/#previous-work","text":"O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.","title":"Previous Work"},{"location":"word2vec/word2vec/#nnlmfeedforward-neural-network-language-model","text":"As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary.","title":"NNLM(feedforward neural network language model)"},{"location":"word2vec/word2vec/#new-log-linear-models","text":"","title":"New Log-linear Models"},{"location":"word2vec/word2vec/#cbow","text":"Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V)","title":"CBOW"},{"location":"word2vec/word2vec/#skip-gram","text":"Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"Skip-gram"},{"location":"zfs/zfs/","text":"Zettabyte File System A new file system with strong data integrity, simple administration and immense capacity. Features redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data Concepts On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself. Implementation - SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects SPA (Storage Pool Allocator) verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks) DMU (Data Management Unit) when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes ZPL (ZFS Posix Layer) use intent log to avoid losing writes before system crashes","title":"zfs"},{"location":"zfs/zfs/#zettabyte-file-system","text":"A new file system with strong data integrity, simple administration and immense capacity.","title":"Zettabyte File System"},{"location":"zfs/zfs/#features","text":"redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data","title":"Features"},{"location":"zfs/zfs/#concepts","text":"On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself.","title":"Concepts"},{"location":"zfs/zfs/#implementation","text":"- SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects","title":"Implementation"},{"location":"zfs/zfs/#spa-storage-pool-allocator","text":"verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks)","title":"SPA (Storage Pool Allocator)"},{"location":"zfs/zfs/#dmu-data-management-unit","text":"when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes","title":"DMU (Data Management Unit)"},{"location":"zfs/zfs/#zpl-zfs-posix-layer","text":"use intent log to avoid losing writes before system crashes","title":"ZPL (ZFS Posix Layer)"},{"location":"zookeeper/zookeeper/","text":"ZooKeeper: Wait-free coordination for Internet-scale systems A service for coordinating processes of distributed applications. Basics ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"zookeeper"},{"location":"zookeeper/zookeeper/#zookeeper-wait-free-coordination-for-internet-scale-systems","text":"A service for coordinating processes of distributed applications.","title":"ZooKeeper: Wait-free coordination for Internet-scale systems"},{"location":"zookeeper/zookeeper/#basics","text":"ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1.","title":"Basics"},{"location":"zookeeper/zookeeper/#service-overview","text":"","title":"Service Overview"},{"location":"zookeeper/zookeeper/#znode","text":"ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB).","title":"znode"},{"location":"zookeeper/zookeeper/#client-api","text":"ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages.","title":"Client API"},{"location":"zookeeper/zookeeper/#implementation","text":"ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"Implementation"}]}