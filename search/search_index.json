{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A future is not given to you. It is something you must take for yourself.","title":"Home"},{"location":"bigtable/bigtable/","text":"Bigtable A distributed storage system for managing structured data that is designed to scale to a very large size. Data Model Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded Implementation Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry) Refinements locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"bigtable"},{"location":"bigtable/bigtable/#bigtable","text":"A distributed storage system for managing structured data that is designed to scale to a very large size.","title":"Bigtable"},{"location":"bigtable/bigtable/#data-model","text":"Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded","title":"Data Model"},{"location":"bigtable/bigtable/#implementation","text":"Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry)","title":"Implementation"},{"location":"bigtable/bigtable/#refinements","text":"locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"Refinements"},{"location":"borg/borg/","text":"Large-scale cluster management at Google with Borg A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization. Basics Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint. Architecture A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets. Features Scalability To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score. Availability Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down. Isolation Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"borg"},{"location":"borg/borg/#large-scale-cluster-management-at-google-with-borg","text":"A cluster manager runs hundreds of thousands of jobs across clusters effectively and achieves high availability and utilization.","title":"Large-scale cluster management at Google with Borg"},{"location":"borg/borg/#basics","text":"Borg admits, schedules, starts, restarts, and monitors the full range of applications that Google runs. Borg runs mainly two types of workloads, long-running server jobs and batch jobs. A cluster lives inside a datacenter and hosts a Borg cell with median size of 10k heterogenous machines. A Borg task maps to a set of Linux processes running in a container on a machine. Borg uses quota for admission control and priority for scheduling. Borg name service(BNS) writes task's hostname and port to Chubby so RPC system can find task endpoint.","title":"Basics"},{"location":"borg/borg/#architecture","text":"A Borg cell has a logically centralized controller Borgmaster and each machine has an agent process Borglet. Borgmaster consists of two processes: the main Borgmaster process handles client RPCs, manages state machines and communicates with Borglets, the scheduler process assigns tasks to machines with feasibility checking and scoring. The Borgmaster is replicated five times and each replica records states in the Paxos store on local disks. Borgmaster keeps checkpoints(periodic snapshots plus change log) in the Paxos store. Borglet starts/stops tasks and manages local resources. Borgmaster periodically polls Borglets for current state and sends requests. The elected master is responsible for preparing messages to Borglets and updating the cell's state. For performance scalability each Borgmaster runs a stateless link shard to handle the communication with some Borglets.","title":"Architecture"},{"location":"borg/borg/#features","text":"","title":"Features"},{"location":"borg/borg/#scalability","text":"To handle larger cells, split scheduler into a separate process so it can run in parallel with other Borgmaster functions. Use separate threads to talk to Borglets and shard functions across Borgmaster replicas. Cache scores until machine/task properties change. Ignore small changes to reduce cache invalidation. Instead of examining all machines, scheduler examines machines in random order until it finds enough feasible machines to score.","title":"Scalability"},{"location":"borg/borg/#availability","text":"Automatically reschedules evicted tasks. Reduce correlated failures by spreading tasks of a job across failure domains. Use declarative desired-state representations and idempotent mutating operations. Already-running tasks continue even if the Borgmaster or the task's Borglet goes down.","title":"Availability"},{"location":"borg/borg/#isolation","text":"Security isolation is implemented with Linux chroot jail. Performance isolation is implemented with Linux control groups(cgroups). When running out of non-compressible resources(memory, disk, etc), the Borglet terminates task from low priority to high. When running out of compressible resources(CPU cycles, I/O bandwidth, etc), the Borglet throttles usage of not latency-sensitive applications.","title":"Isolation"},{"location":"f1/f1/","text":"F1: A Distributed SQL Database That Scales A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL. Basics MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency. Architecture F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers. Features Hierarchical Schema Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser). Non-blocking Schema Changes A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies. Optimistic Transaction F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions. Change History Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache. Query Processing F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"f1"},{"location":"f1/f1/#f1-a-distributed-sql-database-that-scales","text":"A hybrid database that combines high availability, the scalability of NoSQL and consistency of traditional SQL.","title":"F1: A Distributed SQL Database That Scales"},{"location":"f1/f1/#basics","text":"MySQL can't to meet scalability and reliability requirements as it is hard to scale up and rebalance. F1 is built on Spanner, which provides scalable data storage, synchronous replication, strong consistency and ordering properties. F1 has higher latency for typical reads and writes, so many techniques(even a new ORM layer) are used to hide the latency.","title":"Basics"},{"location":"f1/f1/#architecture","text":"F1 servers are typically co-located in the datacenters as the Spanner servers storing the data. F1 servers can communicate with Spanner servers outside their own datacenters for availability and load balancing. Shared slave pool consists of F1 processes to execute distributed query plans. F1 master monitors slave process health and distributes the list of available slaves to F1 servers.","title":"Architecture"},{"location":"f1/f1/#features","text":"","title":"Features"},{"location":"f1/f1/#hierarchical-schema","text":"Logically, tables in the F1 schema can be organized into a hierarchy. Physically, F1 stores each child table clustered with and interleaved within the rows from its parent table(the child table must have a foreign key to its parent table as a prefix of its primary key). Reading all AdGroups for a Customer can be expressed as a single range read. Hierarchical clustering is useful for updates since it reduces the number of Spanner groups involved in a transaction. Flat schema is possible, but hierarchy matching data semantics is beneficial(most transactions update data for a single advertiser).","title":"Hierarchical Schema"},{"location":"f1/f1/#non-blocking-schema-changes","text":"A schema change algorithm that: Enforcing across all F1 servers, at most twoschemas(\"current\" and \"next\") are active. Dividing schema change into multiple phases where consecutive pair of phases are compatible and cannot cause anomalies.","title":"Non-blocking Schema Changes"},{"location":"f1/f1/#optimistic-transaction","text":"F1 implements three types of transactions: snapshot(read-only). pessimistic(Spanner transaction that requires lock). optimistic(arbitrary long read phase without lock + short write phase with lock). Benefits: tolerating misbehaved clients, long-lasting transactions, server-side retriability, server failover, speculative writes. Drawbacks: insertion phantoms, low throughput under high contentions.","title":"Optimistic Transaction"},{"location":"f1/f1/#change-history","text":"Every transaction creates ChangeBatch protocol buffers and they are written to change history tables that exist as children of root tables. Change History can be used to trigger incremental processing when root rows change or update cache.","title":"Change History"},{"location":"f1/f1/#query-processing","text":"F1 supports both centralized and distributed execution of queries. Centralized execution is for OLTP queries and runs on one F1 server. Distributed execution is for OLAP queries and spreads workload in the slave pool. F1 mainly uses remote data source, so it optimizes to reduce network latency with batching or pipelining data access(example: Lookup Join operator), while traditional database optimizes to reduce disk latency. F1 operators stream data as much as possible at the cost of not preserving data orders. F1 uses only hash partitioning. Range partitioning is infeasible because Spanner applies random partitioning. F1 operators execute in memory without checkpointing to disk, so queries run fast but will fail when any part fails(transparent retry hides these failures). Protocol buffers have performance implications since we have to fetch and parse entire message even when using a subset of fields.","title":"Query Processing"},{"location":"flume/flume/","text":"FlumeJava: Easy, Efficient Data-Parallel Pipelines The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. VS MapReduce Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed. Basics Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> . Optimizer ParallelDo Fusion Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) . MSCR Fusion MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces. Strategy The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs. Executor Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"flume"},{"location":"flume/flume/#flumejava-easy-efficient-data-parallel-pipelines","text":"The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines.","title":"FlumeJava: Easy, Efficient Data-Parallel Pipelines"},{"location":"flume/flume/#vs-mapreduce","text":"Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed.","title":"VS MapReduce"},{"location":"flume/flume/#basics","text":"Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> .","title":"Basics"},{"location":"flume/flume/#optimizer","text":"","title":"Optimizer"},{"location":"flume/flume/#paralleldo-fusion","text":"Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) .","title":"ParallelDo Fusion"},{"location":"flume/flume/#mscr-fusion","text":"MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces.","title":"MSCR Fusion"},{"location":"flume/flume/#strategy","text":"The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs.","title":"Strategy"},{"location":"flume/flume/#executor","text":"Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"Executor"},{"location":"google-news/google-news/","text":"Google News Personalization: Scalable Online Collaborative Filtering Collaborative filtering for generating personalized recommendations for users of Google News. Background Algorithms MinHash PLSI Covisitation System Structure","title":"Google News Personalization: Scalable Online Collaborative Filtering"},{"location":"google-news/google-news/#google-news-personalization-scalable-online-collaborative-filtering","text":"Collaborative filtering for generating personalized recommendations for users of Google News.","title":"Google News Personalization: Scalable Online Collaborative Filtering"},{"location":"google-news/google-news/#background","text":"","title":"Background"},{"location":"google-news/google-news/#algorithms","text":"","title":"Algorithms"},{"location":"google-news/google-news/#minhash","text":"","title":"MinHash"},{"location":"google-news/google-news/#plsi","text":"","title":"PLSI"},{"location":"google-news/google-news/#covisitation","text":"","title":"Covisitation"},{"location":"google-news/google-news/#system-structure","text":"","title":"System Structure"},{"location":"mapreduce/mapreduce/","text":"MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets. Introduction Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Execution A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks. Fault Tolerance For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry. Miscellaneous Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers.","title":"mapreduce"},{"location":"mapreduce/mapreduce/#mapreduce-simplified-data-processing-on-large-clusters","text":"A programming model and an associated implementation for processing and generating large data sets.","title":"MapReduce: Simplified Data Processing on Large Clusters"},{"location":"mapreduce/mapreduce/#introduction","text":"Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key.","title":"Introduction"},{"location":"mapreduce/mapreduce/#execution","text":"A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks.","title":"Execution"},{"location":"mapreduce/mapreduce/#fault-tolerance","text":"For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry.","title":"Fault Tolerance"},{"location":"mapreduce/mapreduce/#miscellaneous","text":"Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers.","title":"Miscellaneous"},{"location":"predicting-clicks/predicting-clicks/","text":"Predicting Clicks: Estimating the Click-Through Rate for New Ads Use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. Background The key task for a search engine advertising system is to determine what advertisements should be displayed, and in what order, for each query that the search engine receives. Assume the probability of an ad is clicked is independent of its position and the probability of an ad is viewed only depends on position. P(click|ad,pos)=p(click|ad, seen) \\cdot p(seen|pos) . Then CTR is defined as p(click|ad, seen) . For ads that has been displayed enough times(up to 200-300 search pages), CTR can be estimated based on historical information, but this doesn't work for new ads. Model Basics The goal is to create a model to predict initial CTR for new ads. Training and testing data is split on advertiser level, so we can consider each ad and account as completely novel to the system. Use Logistics regression and cross-entropy loss function. For each feature f_i , add derived features log(f_i+1) and fi^2 . Feature values more than five standard deviations from the mean are truncated. Features Term CTR: the CTR of other ads with same or related bid terms. Ad quality feature set manual features: appearance, attention capture, reputation, landing page quality, relevance. unigram features: one-hot encoding of most common 10K words in ad title and body. Order specificity feature set: capture how targeted an order with the category entropy of bid terms. Search data feature set: the approximate frequency of bid term occurring on the Web and users query for the bid term. Others Although there is overlapping between features, we want to Include as many feature sets as possible for robustness in adversarial situations. Improvements: making the CTR estimation dependent on user query, adding human judges as new source of information, making the model time-dependent.","title":"predicting-clicks"},{"location":"predicting-clicks/predicting-clicks/#predicting-clicks-estimating-the-click-through-rate-for-new-ads","text":"Use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads.","title":"Predicting Clicks: Estimating the Click-Through Rate for New Ads"},{"location":"predicting-clicks/predicting-clicks/#background","text":"The key task for a search engine advertising system is to determine what advertisements should be displayed, and in what order, for each query that the search engine receives. Assume the probability of an ad is clicked is independent of its position and the probability of an ad is viewed only depends on position. P(click|ad,pos)=p(click|ad, seen) \\cdot p(seen|pos) . Then CTR is defined as p(click|ad, seen) . For ads that has been displayed enough times(up to 200-300 search pages), CTR can be estimated based on historical information, but this doesn't work for new ads.","title":"Background"},{"location":"predicting-clicks/predicting-clicks/#model-basics","text":"The goal is to create a model to predict initial CTR for new ads. Training and testing data is split on advertiser level, so we can consider each ad and account as completely novel to the system. Use Logistics regression and cross-entropy loss function. For each feature f_i , add derived features log(f_i+1) and fi^2 . Feature values more than five standard deviations from the mean are truncated.","title":"Model Basics"},{"location":"predicting-clicks/predicting-clicks/#features","text":"Term CTR: the CTR of other ads with same or related bid terms. Ad quality feature set manual features: appearance, attention capture, reputation, landing page quality, relevance. unigram features: one-hot encoding of most common 10K words in ad title and body. Order specificity feature set: capture how targeted an order with the category entropy of bid terms. Search data feature set: the approximate frequency of bid term occurring on the Web and users query for the bid term.","title":"Features"},{"location":"predicting-clicks/predicting-clicks/#others","text":"Although there is overlapping between features, we want to Include as many feature sets as possible for robustness in adversarial situations. Improvements: making the CTR estimation dependent on user query, adding human judges as new source of information, making the model time-dependent.","title":"Others"},{"location":"rdd/rdd/","text":"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing A distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Background Current distributed computing frameworks(like MapReduce) lack abstractions for leveraging distributed memory, so they are inefficient for applications that need to reuse intermediate results(iterative machine learning, interactive data mining, etc). Current abstractions for cluster in-memory storage(like Piccolo) provide interface with fine-grained updates. They have to replicate data or log updates across machines for fault tolerance, which is expensive for data-intensive workloads. RDD Model An RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on (1) data in stable storage or (2) other RDDs. RDD provides interface based on coarse-grained transformations that apply the same operation to many data items, which allows efficient fault tolerance by logging the transformations used to build a dataset(lineage) rather than actual data. RDD is not suitable for applications that needs to make asynchronous fine-grained updates to shared states(incremental web crawler, etc). Implementation An RDD is represented as (1) a set of partitions, (2) a set of dependencies of parent RDDs, (3) a function for computing the dataset from parents and (4) metadata about partitioning scheme and data placement. There are two types of dependencies, narrow(like map or filter) and wide(like join). Narrow dependencies can be more efficiently executed and recovered after failure. The scheduler examines the RDD's lineage graph to build a DAG of stages to execute. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of stages are shuffle operations or any already computed partitions. The scheduler assigns tasks to machines based on data locality using delay scheduling. A task is sent to node that has the partition available in memory or is preferred location for HDFS. If a task fails, the scheduler rerun it on another node and resubmit tasks for parent partitions if necessary. Use LRU eviction policy at the level of RDDs to manage the limited memory available. Checkpointing is useful for RDDs with long lineage graphs containing wide dependencies. The read-only nature of RDDs make them simpler to checkpoint than distributed shared memory.","title":"rdd"},{"location":"rdd/rdd/#resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing","text":"A distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner.","title":"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing"},{"location":"rdd/rdd/#background","text":"Current distributed computing frameworks(like MapReduce) lack abstractions for leveraging distributed memory, so they are inefficient for applications that need to reuse intermediate results(iterative machine learning, interactive data mining, etc). Current abstractions for cluster in-memory storage(like Piccolo) provide interface with fine-grained updates. They have to replicate data or log updates across machines for fault tolerance, which is expensive for data-intensive workloads.","title":"Background"},{"location":"rdd/rdd/#rdd-model","text":"An RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on (1) data in stable storage or (2) other RDDs. RDD provides interface based on coarse-grained transformations that apply the same operation to many data items, which allows efficient fault tolerance by logging the transformations used to build a dataset(lineage) rather than actual data. RDD is not suitable for applications that needs to make asynchronous fine-grained updates to shared states(incremental web crawler, etc).","title":"RDD Model"},{"location":"rdd/rdd/#implementation","text":"An RDD is represented as (1) a set of partitions, (2) a set of dependencies of parent RDDs, (3) a function for computing the dataset from parents and (4) metadata about partitioning scheme and data placement. There are two types of dependencies, narrow(like map or filter) and wide(like join). Narrow dependencies can be more efficiently executed and recovered after failure. The scheduler examines the RDD's lineage graph to build a DAG of stages to execute. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of stages are shuffle operations or any already computed partitions. The scheduler assigns tasks to machines based on data locality using delay scheduling. A task is sent to node that has the partition available in memory or is preferred location for HDFS. If a task fails, the scheduler rerun it on another node and resubmit tasks for parent partitions if necessary. Use LRU eviction policy at the level of RDDs to manage the limited memory available. Checkpointing is useful for RDDs with long lineage graphs containing wide dependencies. The read-only nature of RDDs make them simpler to checkpoint than distributed shared memory.","title":"Implementation"},{"location":"storm/storm/","text":"Storm @Twitter A real-time fault-tolerant and distributed stream data processing system. Basics Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka. Architecture Overview Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism. Nimbus Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks. Workers Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue. Processing Semantics Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"storm"},{"location":"storm/storm/#storm-twitter","text":"A real-time fault-tolerant and distributed stream data processing system.","title":"Storm @Twitter"},{"location":"storm/storm/#basics","text":"Storm data processing architecture consists of streams of tuples flowing through topologies. A topology is a directed graph where vertices represent computation and edges represent the data flow. Storm is used by groups inside Twitter like revenue, user services, search and content discovery. Most of topologies have <3 stages. Storm runs on distributed cluster like Mesos, uses ZooKeeper to keep state and pulls data from queues like Kafka.","title":"Basics"},{"location":"storm/storm/#architecture","text":"","title":"Architecture"},{"location":"storm/storm/#overview","text":"Clients submit topologies to Nimbus, the master node of Storm. Nimbus distributes and coordinates the execution of topologies on worker nodes. Each worker node runs a Supervisor, which receives assignment from Nimbus and monitors the health of workers. Each worker node runs one or more worker processes. Each work process runs a JVM, in which it runs one or more executors made of tasks. Worker process serve as containers on host machines. Tasks provide intra-bolt/intra-spout parallelism and executors provide intra-topology parallelism.","title":"Overview"},{"location":"storm/storm/#nimbus","text":"Nimbus is a Thrift service. Topologies are Thrift objects. Nimbus stores topologies in ZooKeeper and Jar files of user codes on local disk of the Nimbus machine. Nimbus and Supervisor are fail-fast and stateless. States are kept in ZooKeeper or local disks.","title":"Nimbus"},{"location":"storm/storm/#workers","text":"Each worker has two dedicated threads, a worker receive thread and a worker send thread, to route incoming and outgoing tuples. User logic thread runs actual task for input tuples and places output tuples to output queue. Executor send thread checks the task identifier of tuples in output queue and writes them to corresponding input queue(destination is same worker) or global transfer queue.","title":"Workers"},{"location":"storm/storm/#processing-semantics","text":"Storm provides two processing semantics, \"at least once\" and \"at most once\". \"At least once\" is done by an \"acker\" bolt. It tracks the DAG of every tuple emitted by spout and acknowledges tasks when output tuple leaves topology. Storm generates 64-bit message id for each tuple. Acker bolt uses XOR checksum of message ids to avoid large memory usage for provenance tracking.","title":"Processing Semantics"},{"location":"word2vec/word2vec/","text":"Efficient Estimation of Word Representations in Vector Space Novel model architectures for computing continuous vector representations of words from very large data sets. Background Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\"). Previous Work O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific. NNLM(feedforward neural network language model) As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary. New Log-linear Models CBOW Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V) Skip-gram Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"word2vec"},{"location":"word2vec/word2vec/#efficient-estimation-of-word-representations-in-vector-space","text":"Novel model architectures for computing continuous vector representations of words from very large data sets.","title":"Efficient Estimation of Word Representations in Vector Space"},{"location":"word2vec/word2vec/#background","text":"Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\").","title":"Background"},{"location":"word2vec/word2vec/#previous-work","text":"O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.","title":"Previous Work"},{"location":"word2vec/word2vec/#nnlmfeedforward-neural-network-language-model","text":"As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary.","title":"NNLM(feedforward neural network language model)"},{"location":"word2vec/word2vec/#new-log-linear-models","text":"","title":"New Log-linear Models"},{"location":"word2vec/word2vec/#cbow","text":"Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V)","title":"CBOW"},{"location":"word2vec/word2vec/#skip-gram","text":"Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"Skip-gram"},{"location":"zfs/zfs/","text":"Zettabyte File System A new file system with strong data integrity, simple administration and immense capacity. Features redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data Concepts On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself. Implementation - SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects SPA (Storage Pool Allocator) verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks) DMU (Data Management Unit) when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes ZPL (ZFS Posix Layer) use intent log to avoid losing writes before system crashes","title":"zfs"},{"location":"zfs/zfs/#zettabyte-file-system","text":"A new file system with strong data integrity, simple administration and immense capacity.","title":"Zettabyte File System"},{"location":"zfs/zfs/#features","text":"redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data","title":"Features"},{"location":"zfs/zfs/#concepts","text":"On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself.","title":"Concepts"},{"location":"zfs/zfs/#implementation","text":"- SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects","title":"Implementation"},{"location":"zfs/zfs/#spa-storage-pool-allocator","text":"verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks)","title":"SPA (Storage Pool Allocator)"},{"location":"zfs/zfs/#dmu-data-management-unit","text":"when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes","title":"DMU (Data Management Unit)"},{"location":"zfs/zfs/#zpl-zfs-posix-layer","text":"use intent log to avoid losing writes before system crashes","title":"ZPL (ZFS Posix Layer)"},{"location":"zookeeper/zookeeper/","text":"ZooKeeper: Wait-free coordination for Internet-scale systems A service for coordinating processes of distributed applications. Basics ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"zookeeper"},{"location":"zookeeper/zookeeper/#zookeeper-wait-free-coordination-for-internet-scale-systems","text":"A service for coordinating processes of distributed applications.","title":"ZooKeeper: Wait-free coordination for Internet-scale systems"},{"location":"zookeeper/zookeeper/#basics","text":"ZooKeeper provides a coordination kernel for clients to implement primitives for configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper guarantees FIFO client ordering for all operations and linearizable writes. ZooKeeper target workload read to write ration is 2:1 to 100:1.","title":"Basics"},{"location":"zookeeper/zookeeper/#service-overview","text":"","title":"Service Overview"},{"location":"zookeeper/zookeeper/#znode","text":"ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. A znode can be regular or ephemeral(automatically removed when corresponding session terminates). znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB).","title":"znode"},{"location":"zookeeper/zookeeper/#client-api","text":"ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages.","title":"Client API"},{"location":"zookeeper/zookeeper/#implementation","text":"ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. ZooKeeper state is not locked when taking the snapshot, but idempotent transactions can be applied twice as long as in order.","title":"Implementation"}]}