{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A future is not given to you. It is something you must take for yourself.","title":"Home"},{"location":"bigtable/bigtable/","text":"Bigtable A distributed storage system for managing structured data that is designed to scale to a very large size. Data Model Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded Implementation Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry) Refinements locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"bigtable"},{"location":"bigtable/bigtable/#bigtable","text":"A distributed storage system for managing structured data that is designed to scale to a very large size.","title":"Bigtable"},{"location":"bigtable/bigtable/#data-model","text":"Rows read/write under a row key is atomic row keys are in lexicographic order tablet(row range) is unit of distribution and load balancing Column families basic unit of access control column family number is small and rarely change, while column number is unbounded","title":"Data Model"},{"location":"bigtable/bigtable/#implementation","text":"Tablet Location three-level hierarchy: Chubby file for root tablet, METADATA tablets, user table client library caches tablet location(search recursively if cache is invalid) Tablet Assignment tablet server creates file in Chubby directory; master monitors the directory to discover tablet server when master starts, it creates file in Chubby directory, communicates with live tablet servers and scans METADATA table Tablet Serving updates: commit log -> memtable -> SSTables METADATA table stores metadata to recover a tablet(SSTables and redo points) Compaction minor compaction: when memtable size reaches threshold, convert current memtable into a SSTable and make a new one merging compaction: periodically merges a few SSTables and memtable into a SSTable major compaction: merges all SSTables into a SSTable(no deletion entry)","title":"Implementation"},{"location":"bigtable/bigtable/#refinements","text":"locality groups: group multiple column families into locality group; seprate locality groups into different SSTables compression bloom filter commit-log use single commit log per tablet server, co-mingling mutations for different tablets For recovery, the commit log is sorted first, so new tablet server doesn't need to read full log file","title":"Refinements"},{"location":"flume/flume/","text":"FlumeJava: Easy, Efficient Data-Parallel Pipelines The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. VS MapReduce Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed. Basics Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> . Optimizer ParallelDo Fusion Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) . MSCR Fusion MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces. Strategy The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs. Executor Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"flume"},{"location":"flume/flume/#flumejava-easy-efficient-data-parallel-pipelines","text":"The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines.","title":"FlumeJava: Easy, Efficient Data-Parallel Pipelines"},{"location":"flume/flume/#vs-mapreduce","text":"Real-life computations require a chain of MapReduce stages. FlumeJava can optimize the execution plan and choose implementation strategy(local loop, remote MapReduce, etc) when running the execution plan. FlumeJava is easier to develop and test. Parallel collections abstract away the details of data representation and parallel operations abstract away the implementation strategy. FlumeJava automatically deletes temporary intermediate files when no longer needed.","title":"VS MapReduce"},{"location":"flume/flume/#basics","text":"Data: immutable bag of elements PCollection<T> . immutable multi-map PTable<K, V> . Operations: parallelDo for map/reduce. groupByKey for shuffle. combineValues is a special case of parallelDo . It is more efficient since MapReduce combiner is allowed. flatten views a list of PCollection<T> as a single PCollection<T> (no copy). join returns PTable<K, Tuple<Collection<V1>, Collection<V2>>> and is implemented with intermediate type PTable<K, TaggedUnion2<V1, V2>> .","title":"Basics"},{"location":"flume/flume/#optimizer","text":"","title":"Optimizer"},{"location":"flume/flume/#paralleldo-fusion","text":"Producer-consumer fusion: replace f(g(x)) with (g + f \\circ g)(x) . Sibling fusion: replace f(x) + g(x) with (f+g)(x) .","title":"ParallelDo Fusion"},{"location":"flume/flume/#mscr-fusion","text":"MSCR(MapShuffleCombineReduce) operation is the intermediate operation to help bridge the gap between(1) combinations of operations and (2) single MapReduces.","title":"MSCR Fusion"},{"location":"flume/flume/#strategy","text":"The optimizers performs multiple passes over the execution plan to produce the fewest, most efficient MSCR operations. Sink Flattens: create opportunities for ParallelDo fusion. Lift CombineValues operations: CombineValues immediately follows GroupByKey is subject to ParallelDo fusion. Insert fusion blocks: for ParallelDos between two GroupByKeys, FlumeJava needs to estimate size of intermediate output and mark boundary to block ParallelDo fusion. Fuse ParallelDos. Fuse MSCRs.","title":"Strategy"},{"location":"flume/flume/#executor","text":"Batch execution: FlumeJava traverses the operations in the execution plan in forward topological order.Independent operations are operated simultaneously.","title":"Executor"},{"location":"word2vec/word2vec/","text":"Efficient Estimation of Word Representations in Vector Space Novel model architectures for computing continuous vector representations of words from very large data sets. Background Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\"). Previous Work O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific. NNLM(feedforward neural network language model) As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary. New Log-linear Models CBOW Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V) Skip-gram Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"word2vec"},{"location":"word2vec/word2vec/#efficient-estimation-of-word-representations-in-vector-space","text":"Novel model architectures for computing continuous vector representations of words from very large data sets.","title":"Efficient Estimation of Word Representations in Vector Space"},{"location":"word2vec/word2vec/#background","text":"Many current NLP systems treat words as atomic units, without notion of similarity. We want to preserve the linear regularities among words, vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\").","title":"Background"},{"location":"word2vec/word2vec/#previous-work","text":"O= E \\times T \\times Q O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.","title":"Previous Work"},{"location":"word2vec/word2vec/#nnlmfeedforward-neural-network-language-model","text":"As described in \"A Neural Probabilistic Language Model\", N previous words are encoded, projected and concatenated together in the project layer. Q=N \\times D + N \\times D \\times H + H \\times V The dominating term is N \\times D \\times H because last term can be reduce to H \\times \\log_2(V) with binary tree representations of vocabulary.","title":"NNLM(feedforward neural network language model)"},{"location":"word2vec/word2vec/#new-log-linear-models","text":"","title":"New Log-linear Models"},{"location":"word2vec/word2vec/#cbow","text":"Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection. Q=N \\times D + D \\times \\log_2(V)","title":"CBOW"},{"location":"word2vec/word2vec/#skip-gram","text":"Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors. Q = C \\times (D + D \\times \\log_2(V))","title":"Skip-gram"},{"location":"zfs/zfs/","text":"Zettabyte File System A new file system with strong data integrity, simple administration and immense capacity. Features redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data Concepts On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself. Implementation - SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects SPA (Storage Pool Allocator) verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks) DMU (Data Management Unit) when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes ZPL (ZFS Posix Layer) use intent log to avoid losing writes before system crashes","title":"zfs"},{"location":"zfs/zfs/#zettabyte-file-system","text":"A new file system with strong data integrity, simple administration and immense capacity.","title":"Zettabyte File System"},{"location":"zfs/zfs/#features","text":"redivision of labor between file system and volume manager pooled storage multiple file systems share a pool of storage devices decouple file systems from physical storage immense capacity 128-bit block addresses always consistent on-disk data transactional copy-on-write model data integrity checksum all on-disk data","title":"Features"},{"location":"zfs/zfs/#concepts","text":"On-disk data and metadata are stored in a tree of blocks rooted at uberblock . Data blocks are the leaves of the tree. Each block is checksummed before written to disk. The checksum is stored in the block's parent indirect block. Uberblock stores checksum in itself.","title":"Concepts"},{"location":"zfs/zfs/#implementation","text":"- SPA handles block allocation and I/O; exports virtually addressed, explicitly allocated and freed blocks to DMU - DMU turns virtually addressed blocks into transactional object interface - ZPL implements posix file system on DMU objects","title":"Implementation"},{"location":"zfs/zfs/#spa-storage-pool-allocator","text":"verify block checksum when reading and update when writing use slab allocator to prevent memory fragmentation (copy-on-write file system needs big contiguous chunks to write new blocks)","title":"SPA (Storage Pool Allocator)"},{"location":"zfs/zfs/#dmu-data-management-unit","text":"when a block is written, allocate a new block and copy modified content into the new block transaction is \"rippling\" copy-on-write from data block to uberblock group transactions together so uberblock and indirect blocks can be rewritten once for many data block writes","title":"DMU (Data Management Unit)"},{"location":"zfs/zfs/#zpl-zfs-posix-layer","text":"use intent log to avoid losing writes before system crashes","title":"ZPL (ZFS Posix Layer)"}]}