<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>word2vec - kophy's notes</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Efficient Estimation of Word Representations in Vector Space", url: "#_top", children: [
              {title: "Background", url: "#background" },
              {title: "Previous Work", url: "#previous-work" },
              {title: "New Log-linear Models", url: "#new-log-linear-models" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../zfs/zfs/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../zfs/zfs/" class="btn btn-xs btn-link">
        zfs
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../bigtable/bigtable/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../bigtable/bigtable/" class="btn btn-xs btn-link">
        bigtable
      </a>
    </div>
    
  </div>

    

    <h1 id="efficient-estimation-of-word-representations-in-vector-space">Efficient Estimation of Word Representations in Vector Space</h1>
<p><em>Novel model architectures for computing continuous vector representations of words from very large data sets.</em></p>
<h2 id="background">Background</h2>
<ul>
<li>Many current NLP systems treat words as atomic units, without notion of similarity.</li>
<li>We want to preserve the linear regularities among words, vector("King") - vector("Man") + vector("Woman") = vector("Queen").</li>
</ul>
<h2 id="previous-work">Previous Work</h2>
<p>
<script type="math/tex; mode=display">O= E \times T \times Q</script>
</p>
<p>O is training complexity, E is #training epochs, T is #words in training set, Q is model-specific.</p>
<h3 id="nnlmfeedforward-neural-network-language-model">NNLM(feedforward neural network language model)</h3>
<p>As described in "A Neural Probabilistic Language Model", N previous words are encoded, projected and concatenated together in the project layer.</p>
<p><img alt="nnlm" src="../images/nnlm.jpg" /></p>
<p>
<script type="math/tex; mode=display">Q=N \times D + N \times D \times H + H \times V</script>
</p>
<p>The dominating term is <script type="math/tex; mode=display">N \times D \times H</script> because last term can be reduce to <script type="math/tex; mode=display">H \times \log_2(V)</script> with binary tree representations of vocabulary. </p>
<h2 id="new-log-linear-models">New Log-linear Models</h2>
<h3 id="cbow">CBOW</h3>
<p>Surrounding words are projected into the same position and averaged, so the order of words does not influence the projection.</p>
<p><img alt="cbow" src="../images/cbow.jpg" /></p>
<p>
<script type="math/tex; mode=display">Q=N \times D + D \times \log_2(V)</script>
</p>
<h3 id="skip-gram">Skip-gram</h3>
<p>Use current word to predict words within a certain range before and after the current word. Increasing the range improves the quality of resulting word vectors.</p>
<p><img alt="skip-gram" src="../images/skip-gram.jpg" /></p>
<p>
<script type="math/tex; mode=display">Q = C \times (D + D \times \log_2(V))</script>
</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../zfs/zfs/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../zfs/zfs/" class="btn btn-xs btn-link">
        zfs
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../bigtable/bigtable/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../bigtable/bigtable/" class="btn btn-xs btn-link">
        bigtable
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
      <p>
        <a href="https://github.com/kophy/notes/edit/master/docs/word2vec/word2vec.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>